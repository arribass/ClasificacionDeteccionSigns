{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pathlib\n",
    "import shutil\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    shutil.rmtree('dataset_classifying/')\n",
    "    shutil.rmtree('dataset_detection/')\n",
    "except OSError:\n",
    "    pass\n",
    "try:\n",
    "    shutil.rmtree('csvs_non_processed/')\n",
    "    shutil.rmtree('csvs_processed/')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacion de datos sin preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacion de datos con preprocesamiento\n",
    "\n",
    "Para mejorar el clasificador vamos procesar las imagenes para reducir ruido y darles un formato, es decir, un tamaño concreto rango de colores etc.\n",
    "\n",
    "Podriamos agrupar la creacion de todos los datos en una sola celda e incluso agruparlo en un bucle pero lo hemos dejado asi para mostrar el proceso que hemos seguido. \n",
    "\n",
    "- Aumento del contraste\n",
    "- Aumento de rango dinamico\n",
    "- Ecualizacion del histograma\n",
    "- Ajustar aspect ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-137b04aa3e2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrutaDatosCsv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mwr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mwr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[0mmyfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ruta = 'Dataset_traffic_sign/'\n",
    "vectorClases = os.listdir(ruta)\n",
    "nClases = len(vectorClases)\n",
    "csvs_types = ['non_processed','processed']\n",
    "tam = 100\n",
    "t0 = time.time()\n",
    "for clase,carpeta in enumerate(vectorClases):\n",
    "    l_non_proc = []    \n",
    "    l_proc = []\n",
    "    rutaImagenes = ruta+carpeta\n",
    "    vectorImagenes = os.listdir(rutaImagenes)\n",
    "    nImagenes = len(vectorImagenes)\n",
    "    for j in vectorImagenes:\n",
    "        #1-Sin procesar\n",
    "        #Leemos imagen en blanco y negro\n",
    "        rutaImg = rutaImagenes+'/'+j\n",
    "        img = cv2.imread(rutaImg,0)\n",
    "        img = cv2.resize(img,(tam,tam))\n",
    "        #Feature hog \n",
    "        hog_img = hog(img)\n",
    "        l_non_proc.append(hog_img)\n",
    "        \n",
    "        #2-Procesar\n",
    "        #Ecualizacion de histograma\n",
    "        img = cv2.equalizeHist(img)\n",
    "        hog_img = hog(img)\n",
    "        l_proc.append(hog_img)\n",
    "        \n",
    "    listas = {'non_processed':l_non_proc,'processed':l_proc}\n",
    "    #Creamos csv de la clase\n",
    "    for csv_type in csvs_types:\n",
    "        #Establecemos la ruta\n",
    "        csvFolder = 'csvs_'+csv_type+'/'\n",
    "        rutaDatosCsv = csvFolder+'datos'+str(clase)+'.csv'\n",
    "        pathlib.Path(csvFolder).mkdir(parents=True, exist_ok=True) \n",
    "        #Extraemos la lista\n",
    "        datos = np.array(listas[csv_type])\n",
    "        #Añadir la columna de la clase\n",
    "        y = np.full((nImagenes,1),clase)\n",
    "        datos = np.hstack((datos,y))\n",
    "        with open(rutaDatosCsv, 'a+', newline='') as myfile:\n",
    "            wr = csv.writer(myfile)\n",
    "            wr.writerows(datos)\n",
    "            myfile.close()\n",
    "        \n",
    "n_features = datos.shape[1]\n",
    "\n",
    "print('El numero de caracteristicas es {}\\n'.format(n_features))\n",
    "\n",
    "for csv_type in csvs_types:\n",
    "    print(csv_type)\n",
    "    ruta = 'csvs_'+csv_type+'/'\n",
    "    l_train = np.empty((0, n_features))\n",
    "    l_test = np.empty((0, n_features))\n",
    "    for i,csv_string in enumerate(sorted(os.listdir(ruta),key = lambda x: int(x.split('.')[0][len('datos'):]))):\n",
    "        print(\"Creating train and test sets for {}\".format(csv_string))\n",
    "        rutaCsv = ruta+csv_string\n",
    "        #Leemos los datos del csv_i y los guardamos en data\n",
    "        data = np.loadtxt(rutaCsv,delimiter=',')\n",
    "        X= data[:,:-1]\n",
    "        y= data[:,-1].reshape(-1,1)\n",
    "\n",
    "        #Creamos train y test con sklearn\n",
    "        Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=2020)\n",
    "\n",
    "        train = np.hstack((Xtrain,ytrain))\n",
    "        test = np.hstack((Xtest,ytest))\n",
    "        print(\"Total samples {} train samples {} test samples {}\\n\".format(data.shape[0],train.shape[0],test.shape[0]))\n",
    "        l_train = np.append(l_train,train).reshape(-1,n_features)\n",
    "        l_test = np.append(l_test,test).reshape(-1,n_features)\n",
    "        \n",
    "    print(l_train.shape)\n",
    "    print(l_test.shape)\n",
    "    pathlib.Path('dataset_classifying/').mkdir(parents=True, exist_ok=True)\n",
    "    np.save('dataset_classifying/train_signs_classifying_'+csv_type,l_train)\n",
    "    np.save('dataset_classifying/test_signs_classifying_'+csv_type,l_test)\n",
    "    \n",
    "t1 = time.time()\n",
    "print('Time elapsed creating datasets {}'.format(round(t1-t0,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probar un conjunto de clasificadores usando SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.base import clone\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nombresDatasets = ['non_processed','processed']\n",
    "datasets = {}\n",
    "\n",
    "for nombre in nombresDatasets:\n",
    "    dataset_path = 'dataset_classifying/'\n",
    "    pathTrain = dataset_path+'train_signs_classifying_'+nombre+'.npy'\n",
    "    pathTest = dataset_path+'test_signs_classifying_'+nombre+'.npy'\n",
    "    \n",
    "    dataTrain = np.load(pathTrain)\n",
    "    Xtrain = dataTrain[:,:-1]\n",
    "    ytrain = dataTrain[:,-1]\n",
    "    \n",
    "    dataTest = np.load(pathTest)\n",
    "    Xtest = dataTest[:,:-1]\n",
    "    ytest = dataTest[:,-1]\n",
    "    \n",
    "    datasets[nombre] = (Xtrain,ytrain,Xtest,ytest)\n",
    "\n",
    "#Definimos un conjunto de clasificadores y sus nombre para etiquetarlos luego\n",
    "classifiers = [\n",
    "    SVC(),\n",
    "    LogisticRegression(random_state=0,max_iter=400),\n",
    "    GaussianNB(),\n",
    "    DecisionTreeClassifier(),\n",
    "    MLPClassifier()]\n",
    "\n",
    "clf_names = ['SVM', 'Regr Logistica', 'NB Gaussiano','Decision Tree','Red Neuronal']\n",
    "\n",
    "score_list = []\n",
    "time_list = []\n",
    "aux_params = []\n",
    "Best_score = np.NINF\n",
    "\n",
    "#Definimos un diccionario con diccionarios para los parametergrids de GridSearchCV\n",
    "parameters_dict = {'SVM':{'kernel':('linear', 'rbf'), 'C':[1, 10],'gamma':[0.1,0.001]},\n",
    "                    'Regr Logistica':{'C': [0.01, 1, 10, 1000] },\n",
    "                    'NB Gaussiano':{'var_smoothing': np.logspace(0,-9, num=10)},\n",
    "                    'Decision Tree':{'min_samples_split': [2, 3, 4],\n",
    "                                     'criterion': ['gini', 'entropy']},\n",
    "                    'Red Neuronal':{'activation': ['tanh', 'relu'],\n",
    "                                    'solver': ['sgd', 'adam']}\n",
    "                  }\n",
    "#Probamos los clasificadores\n",
    "for i,(clf_aux,clf_name) in enumerate(zip(classifiers,clf_names)):\n",
    "    for j,dataset in enumerate(nombresDatasets):\n",
    "        #Clonamos el clasificador ya que lo vamos a usar 2 veces\n",
    "        clf = clone(clf_aux)\n",
    "        \n",
    "        #Extraemos los datos de train y test\n",
    "        print('Entrenando {} con {}'.format(dataset,clf_name))\n",
    "        (Xtrain,ytrain,Xtest,ytest) = datasets[dataset]\n",
    "        \n",
    "        #Calculamos los parametros optimos con GridSearchCV\n",
    "        t0 = time.time()\n",
    "        clf = GridSearchCV(clf,parameters_dict[clf_name],cv = 2,n_jobs=-1)\n",
    "        clf.fit(Xtrain,ytrain)\n",
    "        t1 = time.time()\n",
    "        time_list.append(round(t1-t0,2))\n",
    "        \n",
    "        #Calculamos la precision, la guardamos y vemos si hemos mejorado\n",
    "        score = round(clf.score(Xtest,ytest)*100,2)\n",
    "        score_list.append(score)\n",
    "        aux_params.append(clf.best_params_)\n",
    "        if score >= Best_score:\n",
    "            Best_dataset = dataset\n",
    "            Best_clf = clf\n",
    "        print('Time elapsed on fit: {}\\n'.format(round(t1-t0,2)))\n",
    "print('Total time elapsed {}'.format(round(np.sum(time_list),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcion para mostrar resultados de score y tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficas_resultados(bar_list,path,width = 0.35,Best = None,title = None):\n",
    "    '''Muestra graficas de resultados'''\n",
    "    def autolabel(rects):\n",
    "        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate('{}'.format(height),\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "        \n",
    "    x = np.arange(len(clf_names))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (10,7))\n",
    "    rects1 = ax.bar(x - width/2, bar_list[::2], width, label='no procesado',edgecolor='k')\n",
    "    rects2 = ax.bar(x + width/2, bar_list[1::2], width, label='procesado',edgecolor='k')\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    if title is not None:\n",
    "        ax.set_ylabel(title)\n",
    "    ax.set_title('Resultados clasificadores')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(clf_names)\n",
    "    ax.text(2.7,77,'verde el mejor',bbox={'facecolor': 'limegreen', 'alpha': 0.5, 'pad': 10})\n",
    "    \n",
    "    autolabel(rects1)\n",
    "    autolabel(rects2)\n",
    "    if Best is not None:\n",
    "        if Best == 'max':\n",
    "            i_clf = np.argmax(bar_list)\n",
    "        else:\n",
    "            i_clf = np.argmin(bar_list)\n",
    "        if i_clf % 2 == 0:\n",
    "            #Sin preprocesado\n",
    "            ic = int(i_clf/2)\n",
    "            Best_dataset = 'non_processed'\n",
    "            rects1[ic].set_color('limegreen')\n",
    "            rects1[ic].set_edgecolor('k')\n",
    "        else:\n",
    "            #Preprocesado\n",
    "            ic = int(i_clf/2)\n",
    "            Best_dataset = 'processed'\n",
    "            if score_list[i_clf] == score_list[i_clf-1]:\n",
    "                rects1[ic].set_color('limegreen')\n",
    "                rects2[ic].set_color('limegreen')\n",
    "                rects1[ic].set_edgecolor('k')\n",
    "                rects2[ic].set_edgecolor('k')\n",
    "            else:\n",
    "                rects2[ic].set_color('limegreen')\n",
    "                rects2[ic].set_edgecolor('k')\n",
    "    fig.tight_layout()\n",
    "    ax.legend(['No procesado','Procesado'],loc=(0.62,0.9))\n",
    "\n",
    "    #Guardamos los mejores resultados\n",
    "    best_result = bar_list[i_clf]\n",
    "    best_name = clf_names[ic]\n",
    "    figpath = title+'_'+str(best_result)+'_'+str(best_name)+'.png'\n",
    "    \n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(path+figpath)\n",
    "    \n",
    "    plt.show()\n",
    "    print('El mejor clasificador ha sido: {} en {}'.format(best_name,Best_dataset))\n",
    "    print('El mejor resultado ha sido {} {} en {}'.format(title,best_result,best_name))\n",
    "    return ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Graficas score\n",
    "#Llamada funcion\n",
    "datepath = 'resources/'+datetime.datetime.now().strftime(\"%d_%m_%Y_%H_%M\")+'/'\n",
    "best_score = graficas_resultados(score_list,path = datepath,title='score',Best='max')\n",
    "best_time = graficas_resultados(time_list,path = datepath,title='tiempo',Best='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos el mejor clasificador en disco para utilizarlo mas adelante\n",
    "import pickle\n",
    "\n",
    "# save the classifier\n",
    "short_clf_names = ['SVM','RegLog','NBGauss','DecTree','RedNeu']\n",
    "ds_type = '_'+Best_dataset\n",
    "classifier_name = 'sign_classifier_'+short_clf_names[best_score]+ds_type+'.pkl'\n",
    "try:\n",
    "    open(classifier_name, 'rb')\n",
    "except:\n",
    "    with open(classifier_name, 'wb') as f:\n",
    "        pickle.dump(Best_clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deteccion de señales\n",
    "\n",
    "Ahora a partir de las imagenes en Dataset_images/ vamos a crear un conjunto de imagenes tomando pequeñas imagenes 100x100 para entrenar un clasificador en este caso de tan solo 2 clases que nos permita averiguar si la imagen contiene o no una señal\n",
    "\n",
    "Para ello vamos a obtener coger la carpeta de [Dataset_images](Dataset_images/) y vamos crear recortes de tamaño 100x100 y lo vamos a combinar con imagenes de la carpeta [Dataset_traffic_sign](Dataset_traffic_sign/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creaRecortes(recortesPorImagen,tamRecortes = 100,delete = 'yes'):\n",
    "    nRecorte = 0\n",
    "    ruta = 'Dataset_images/'\n",
    "    pathlib.Path('cropped_images/').mkdir(parents=True, exist_ok=True)\n",
    "    for i,im_path in enumerate(os.listdir(ruta)):\n",
    "        imagen = cv2.imread(ruta+im_path, 0)\n",
    "        for j in range(0,recortesPorImagen):\n",
    "            im_path_w = 'cropped_images/' + str(nRecorte) + '.jpg'\n",
    "            if not os.path.isfile(im_path_w):\n",
    "                f,c = imagen.shape #800 y 1360\n",
    "                rf = np.random.randint(f-tamRecortes-1) #Valor aleatorio de fila\n",
    "                rc = np.random.randint(c-tamRecortes-1) #Valor aleatorio de columna\n",
    "\n",
    "                # rf,rc es el píxel inicial del recorte\n",
    "                recorte = imagen[rf:rf+tamRecortes, rc:rc+tamRecortes]\n",
    "                cv2.imwrite(im_path_w, recorte)\n",
    "            nRecorte += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos las imagenes\n",
    "creaRecortes(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos creado los recortes vamos a generar un dataset combinando las imagenes NO_SEÑAL de [cropped_images](cropped_images/) y las imagenes de la clase SEÑAL de la carpeta [Dataset_traffic_sign](Dataset_traffic_sign/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creado data_sign (502, 8101)\n",
      "Creado data_no_sign (1000, 8101)\n",
      "Creado data (1502, 8101)\n",
      "(1502, 8101)\n"
     ]
    }
   ],
   "source": [
    "rutas = ['cropped_images/','Dataset_traffic_sign/']\n",
    "lista_sign = []\n",
    "lista_no_sign = []\n",
    "tam = 100\n",
    "\n",
    "#SIGNS\n",
    "ruta = 'Dataset_traffic_sign/'\n",
    "for carpeta in os.listdir(ruta):\n",
    "    #Listamos el directorio\n",
    "    lista_carpeta = os.listdir(ruta+carpeta)\n",
    "    #Vamos a coger la mitad de las imagenes\n",
    "    len_c = int(0.415*len(lista_carpeta))\n",
    "    for img in lista_carpeta[:len_c]:\n",
    "#         print(ruta+carpeta+'/'+img)\n",
    "        #Leemos y hacemos resize\n",
    "        img = cv2.imread(ruta+carpeta+'/'+img,0)\n",
    "        img = cv2.resize(img,(tam,tam))\n",
    "        #Extraemos las caracteristicas con hog\n",
    "        hog_img = hog(img)\n",
    "        lista_sign.append(hog_img)\n",
    "        \n",
    "#Añadimos la columna de clase\n",
    "data_sign = np.array(lista_sign)\n",
    "clase_sign = np.full((data_sign.shape[0],1),1)\n",
    "data_sign = np.hstack((data_sign,clase_sign))\n",
    "print('Creado data_sign {}'.format(data_sign.shape))\n",
    "\n",
    "#NON SIGNS\n",
    "ruta = 'cropped_images/'\n",
    "for img in os.listdir(ruta):\n",
    "#     print(ruta+carpeta+'/'+img)\n",
    "    #Leemos y hacemos resize\n",
    "    img = cv2.imread(ruta+'/'+img,0)\n",
    "    img = cv2.resize(img,(tam,tam))\n",
    "    #Extraemos las caracteristicas con hog\n",
    "    hog_img = hog(img)\n",
    "    lista_no_sign.append(hog_img)\n",
    "        \n",
    "#Añadimos la columna de clase\n",
    "data_no_sign = np.array(lista_no_sign)\n",
    "clase_no_sign = np.full((data_no_sign.shape[0],1),0)\n",
    "data_no_sign = np.hstack((data_no_sign,clase_no_sign))\n",
    "print('Creado data_no_sign {}'.format(data_no_sign.shape))\n",
    "\n",
    "data = np.vstack((data_sign,data_no_sign))\n",
    "#Obtenemos x e y\n",
    "X= data[:,:-1]\n",
    "y= data[:,-1].reshape(-1,1)\n",
    "\n",
    "#Creamos train y test\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=2022)\n",
    "train = np.hstack((Xtrain,ytrain))\n",
    "test = np.hstack((Xtest,ytest))\n",
    "\n",
    "print('Creado data {}'.format(data.shape))\n",
    "print(data.shape)\n",
    "\n",
    "#Guardamos los datasets\n",
    "pathlib.Path('dataset_detection/').mkdir(parents=True, exist_ok=True)\n",
    "np.save('dataset_detection/train_signs_detection',train)\n",
    "np.save('dataset_detection/test_signs_detection',train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('dataset_detection/train_signs_detection.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = data[:,:-1]\n",
    "ytrain = data[:,-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
